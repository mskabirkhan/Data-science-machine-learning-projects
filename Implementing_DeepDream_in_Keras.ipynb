{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing DeepDream in Keras",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLlzjvz2GHTsV6BxQKTHG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahria73/my_learning/blob/master/Implementing_DeepDream_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbp6Nhiu6T6-",
        "colab_type": "text"
      },
      "source": [
        "## Implementing DeepDream in Keras\n",
        "\n",
        "We firstly load the InceptionV3 model which tends to produce some of the best visuals. Feel free to try VGG16, VGG19, Xception and ResNet50.\n",
        "\n",
        "Code obtained and edited from F. Chollet (Created of Keras)\n",
        "- \n",
        "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.2-deep-dream.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7xOQWLu6exN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ab8b72c-9f76-4c24-a073-c6256a929055"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUXMB256N8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "6bd90644-3f38-4ed6-93f2-f7797a9aec36"
      },
      "source": [
        "from keras.applications import inception_v3\n",
        "from keras import backend as K\n",
        "\n",
        "# This setting disables all training specific operations\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "# Load InceptionV3\n",
        "model = inception_v3.InceptionV3(weights = 'imagenet', include_top = False)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKxUrjVl67Gv",
        "colab_type": "text"
      },
      "source": [
        "We create a dictionary of coefficients quantifying, how much the layer’s activation contributes to the loss you’ll seek to maximize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyuf_dU_68TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_contributions = {\n",
        "    'mixed2': 0.7,\n",
        "    'mixed3': 2.2,\n",
        "    'mixed4': 1.2,\n",
        "    'mixed5': .2,\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziM4U7Yg6-8G",
        "colab_type": "text"
      },
      "source": [
        "Define our tensor that contains the maximized Loss (the weighted sum of the L2 norm of the activations of the layer defined above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TmyA6go7AdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map layer names to layer instances\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "# loss defined by adding layer contributions\n",
        "loss = K.variable(0.)\n",
        "\n",
        "for layer_name in layer_contributions:\n",
        "  coeff = layer_contributions[layer_name]\n",
        "  \n",
        "  # activation gets the layer output\n",
        "  activation = layer_dict[layer_name].output\n",
        "  scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
        "\n",
        "  # we add the l2 norm\n",
        "  loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqUcfKLm7FTL",
        "colab_type": "text"
      },
      "source": [
        "Creating the Gradient-Ascent Process\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40qTe0Zc8bhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "33fcd192-edd6-431a-a249-7a2a56fc2abf"
      },
      "source": [
        "# this is the image or 'dream' :) that is stored in this tensor \n",
        "dream = model.input\n",
        "\n",
        "# Obtains the gradients wrt to the loss\n",
        "grads = K.gradients(loss, dream)[0]\n",
        "\n",
        "# Normalizes the gradient \n",
        "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
        "\n",
        "# Creates a Keras function to get the value of the loss & gradients wrt to the input\n",
        "outputs = [loss, grads]\n",
        "fetch_loss_and_grads = K.function([dream], outputs)\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    \"\"\"returns the loss and gradient values\"\"\"\n",
        "    outs = fetch_loss_and_grads([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    return loss_value, grad_values\n",
        "\n",
        "def gradient_ascent(x, iterations, step, max_loss=None):\n",
        "    \"\"\"Implements gradient access for a specified number of iterations\"\"\"\n",
        "    for i in range(iterations):\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        if max_loss is not None and loss_value > max_loss:\n",
        "            break\n",
        "        print('...Loss value at', i, ':', loss_value)\n",
        "        x += step * grad_values\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrR1M8n48gDa",
        "colab_type": "text"
      },
      "source": [
        "Implementing the Deep Dream Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRM96oWt8fBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "cd93f930-60e1-4ef6-efb2-d3b30fe8c30d"
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from keras.preprocessing import image\n",
        "import imageio\n",
        "\n",
        "def resize_img(img, size):\n",
        "    img = np.copy(img)\n",
        "    factors = (1,\n",
        "               float(size[0]) / img.shape[1],\n",
        "               float(size[1]) / img.shape[2], 1)\n",
        "    return scipy.ndimage.zoom(img, factors, order=1)\n",
        "\n",
        "def save_img(img, fname):\n",
        "    pil_img = deprocess_image(np.copy(img))\n",
        "    imageio.imwrite(fname, pil_img)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = image.load_img(image_path)\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    else:\n",
        "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
        "    x /= 2.\n",
        "    x += 0.5\n",
        "    x *= 255.\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "step = 0.01 #Step size for gradient ascent\n",
        "num_octave = 3 #number of octaves to be run\n",
        "octave_scale = 1.4 #this is the scale for each ensuing octive will be 1.4 times large than the previous\n",
        "iterations = 20 #number of gradient ascent operations we execute \n",
        "max_loss = 10.0 #our early stoping metric, if loss is > max_loss we break the gradient ascent loop\n",
        "\n",
        "base_image_path = '/content/image3.jpg'\n",
        "\n",
        "# Load our image \n",
        "img = preprocess_image(base_image_path)\n",
        "\n",
        "# Initialize a list of tuples for our different images sizes/scales \n",
        "original_shape = img.shape[1:3]\n",
        "successive_shapes = [original_shape]\n",
        "for i in range(1, num_octave):\n",
        "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "    successive_shapes.append(shape)\n",
        "\n",
        "# Reverse list of shapes, so that they are in increasing order\n",
        "successive_shapes = successive_shapes[::-1]\n",
        "\n",
        "# Resize the Numpy array of the image to our smallest scale\n",
        "original_img = np.copy(img)\n",
        "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
        "\n",
        "for shape in successive_shapes:\n",
        "    print('Processing image shape', shape)\n",
        "    img = resize_img(img, shape)\n",
        "    img = gradient_ascent(img,\n",
        "                          iterations=iterations,\n",
        "                          step=step,\n",
        "                          max_loss=max_loss)\n",
        "\n",
        "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
        "    same_size_original = resize_img(original_img, shape)\n",
        "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
        "\n",
        "    img += lost_detail\n",
        "    shrunk_original_img = resize_img(original_img, shape)\n",
        "    save_img(img, fname='/content/' + str(shape) + '.png')\n",
        "    \n",
        "save_img(img, fname='/content/final_dream.png')\n",
        "print(\"DeepDreaming Complete\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing image shape (326, 489)\n",
            "...Loss value at 0 : 1.5103047\n",
            "...Loss value at 1 : 1.9625398\n",
            "...Loss value at 2 : 2.5260818\n",
            "...Loss value at 3 : 3.0823298\n",
            "...Loss value at 4 : 3.594502\n",
            "...Loss value at 5 : 4.1202536\n",
            "...Loss value at 6 : 4.6098223\n",
            "...Loss value at 7 : 5.107523\n",
            "...Loss value at 8 : 5.5600667\n",
            "...Loss value at 9 : 5.998066\n",
            "...Loss value at 10 : 6.4316406\n",
            "...Loss value at 11 : 6.84849\n",
            "...Loss value at 12 : 7.2640715\n",
            "...Loss value at 13 : 7.655438\n",
            "...Loss value at 14 : 8.039218\n",
            "...Loss value at 15 : 8.41487\n",
            "...Loss value at 16 : 8.77997\n",
            "...Loss value at 17 : 9.128268\n",
            "...Loss value at 18 : 9.461864\n",
            "...Loss value at 19 : 9.762173\n",
            "Processing image shape (457, 685)\n",
            "...Loss value at 0 : 2.5948558\n",
            "...Loss value at 1 : 3.716318\n",
            "...Loss value at 2 : 4.6364093\n",
            "...Loss value at 3 : 5.384675\n",
            "...Loss value at 4 : 6.096723\n",
            "...Loss value at 5 : 6.7085786\n",
            "...Loss value at 6 : 7.2830124\n",
            "...Loss value at 7 : 7.8145165\n",
            "...Loss value at 8 : 8.309329\n",
            "...Loss value at 9 : 8.769828\n",
            "...Loss value at 10 : 9.207395\n",
            "...Loss value at 11 : 9.632567\n",
            "Processing image shape (640, 960)\n",
            "...Loss value at 0 : 2.6157782\n",
            "...Loss value at 1 : 3.6556532\n",
            "...Loss value at 2 : 4.531271\n",
            "...Loss value at 3 : 5.292661\n",
            "...Loss value at 4 : 5.983645\n",
            "...Loss value at 5 : 6.598051\n",
            "...Loss value at 6 : 7.187379\n",
            "...Loss value at 7 : 7.732328\n",
            "...Loss value at 8 : 8.25863\n",
            "...Loss value at 9 : 8.75831\n",
            "...Loss value at 10 : 9.229617\n",
            "...Loss value at 11 : 9.699335\n",
            "DeepDreaming Complete\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}